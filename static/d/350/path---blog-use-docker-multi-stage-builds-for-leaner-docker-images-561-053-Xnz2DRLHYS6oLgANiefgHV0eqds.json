{"data":{"markdownRemark":{"html":"<p>Traditionally, building Docker images required complex instructions in a single Dockerfile, or sometimes many Dockerfiles, to create a final optimized Docker image. This complexity made it difficult to maintain Dockerfiles and create lean Docker images.</p>\n<p>To alleviate this complexity, Multi-Stage Builds was introduced as a feature in Docker version 17.05. Today, we'll look at a common, but simplified, Docker problem and resolve the complexities of this problem with the help of Multi-Stage Builds.</p>\n<h2>The Problem</h2>\n<p>To better grasp the problem let's see an example. We are building a Docker image to watch for files that are added to and removed from a directory. Here is the Typescript source code for the program.</p>\n<pre><code>import * as chokidar from 'chokidar'\nconst dirToWatch = '/watch'\nconst watcher = chokidar.watch(dirToWatch, {\n  persistent: true\n});\nconst log = console.log.bind(console);\nwatcher\n  .on('add', path => log(`File ${path} has been added`))\n  .on('addDir', path => log(`Directory ${path} has been added`))\n  .on('unlink', path => log(`File ${path} has been removed`))\n  .on('unlinkDir', path => log(`Directory ${path} has been removed`))\n  .on('error', error => {\n    log(`Watcher error: ${error}`)\n    process.exit(1)\n  });\n</code></pre>\n<p>This program is simple. We're using the Chokidar library to achieve the functionality of watching filesystem events. It watches the <code>/watch</code> directory for files and directories that are added and removed. When an event happens, we print a simple message to stdout.</p>\n<p>Here are all of our files to build and run this program:</p>\n<pre><code>index.ts\npackage.json\ntsconfig.json\nDockerfile\n</code></pre>\n<p>We have the source code in <code>index.ts</code>, the NPM dependencies in <code>package.json</code>, the Typescript build configuration in <code>tsconfig.json</code>, and the <code>Dockerfile</code>. Let's edit the Dockerfile to build this image. I'll make sure to add some common, yet suboptimal, operations in this file.</p>\n<pre><code>FROM node:12.13.0-alpine\nWORKDIR /app\nCOPY . .\nRUN npm install --quiet\nRUN ./node_modules/typescript/bin/tsc\nENTRYPOINT [ \"node\" ]\nCMD [\"index.js\"]\n</code></pre>\n<p>Here we perform the following steps:</p>\n<ul>\n<li>use the <code>node:12.13.0-alpine</code> image as the base image. This gives us <code>npm</code> and <code>node</code>.</li>\n<li>set the working directory to <code>/app</code>.</li>\n<li>copy the entire current working directory on the host into the <code>/app</code> directory in the image.</li>\n<li>install the dependencies listed in <code>package.json</code>. This includes some Typescript development dependencies and the Chokidar library.</li>\n<li>compile the Typescript code to Javascript.</li>\n<li>set the Docker entry point and command steps.</li>\n</ul>\n<p>Let's build this image now.</p>\n<pre><code>docker build -t fs-watcher .\n</code></pre>\n<p>On my system, it builds a 144MB Docker image. Not bad but can we do better? Sure. There are two big improvements we can apply:</p>\n<ul>\n<li>Reduce files on disk- only copy into the image the appropriate files that compile the source code.</li>\n<li>Reduce Docker layers- each <code>FROM</code>, <code>COPY</code>, <code>RUN</code>, <code>CMD</code> adds a layer. Each Docker layer adds to the total Docker image size. We can reduce the image size by combining multiple RUN commands into one using multiple `&#x26;&#x26;.</li>\n</ul>\n<p>Let's apply those two things now.</p>\n<pre><code>FROM node:12.13.0-alpine\nWORKDIR /app\nCOPY index.ts package.json tsconfig.json /app/\nRUN npm install --quiet &#x26;&#x26; ./node_modules/typescript/bin/tsc\nENTRYPOINT [ \"node\" ]\nCMD [\"index.js\"]\n</code></pre>\n<p>We now only copied the <code>index.ts</code>, <code>package.json</code>, and <code>tsconfig.json</code> into the image. We've also joined the two <code>RUN</code> commands. Building it now doesn't change the size much (our application is small) but we now have fewer files in the image and one less layer that makes up the Docker image. If this Dockerfile were for a real production application that included more complex steps and we applied the same improvements, you'd see a noticeable decrease in image size.</p>\n<p>The following files now exist within the working directory of the image:</p>\n<pre><code>index.ts\nindex.js\nnode_modules\npackage-lock.json\npackage.json\ntsconfig.json\n</code></pre>\n<p>Better but we can still make improvements. Let's add some steps in the Dockerfile to delete files we no longer need. This includes the NPM files and Typescript source code and configuration files.</p>\n<pre><code>FROM node:12.13.0-alpine\nWORKDIR /app\nCOPY index.ts package.json tsconfig.json /app/\nRUN npm install --quiet &#x26;&#x26; \\\n  ./node_modules/typescript/bin/tsc &#x26;&#x26; \\\n  rm -f index.ts package* tsconfig.json\nENTRYPOINT [ \"node\" ]\nCMD [\"index.js\"]\n</code></pre>\n<p>We add another step in the RUN command to remove all these unnecessary files. We also added a few <code>\\</code> to put commands on each line because the line was getting very long. This is very common in Dockerfiles. After building the image with these changes the following files exist within the working directory of the image:</p>\n<pre><code>index.js\nnode_modules\n</code></pre>\n<p>Sweet! There now is a single entry file and the dependencies it needs. Looking back, the Dockerfile has improved a lot. However, there were a few odd steps we needed to perform to achieve this result. We had to:</p>\n<ul>\n<li>add multiple commands into a single <code>RUN</code> instruction</li>\n<li>make sure each command after the <code>RUN</code> instruction was in the right order</li>\n<li>add multiple <code>\\</code> to break many commands into separate lines</li>\n<li>copy many files into the Docker image</li>\n<li>add <code>rm</code> commands to remove files we no longer needed</li>\n</ul>\n<p>As your application gets more complex, the number of files and commands greatly increases. You end up having to remember to perform a ton of steps and make sure those steps are in the right order. If you aren't careful the complexity of the Dockerfile can contribute to large Docker images.\nI've even seen some codebases with different Dockerfiles based on the environment or need. Dockerfiles depended on others. Each Dockerfile was a little different and it would produce an image that copied dependencies from other images. Some Dockerfiles used volumes and mount points to allow the host to access the binaries or dependencies produced by the image so they could be copied to other images. It was a mess. It worked but it felt wrong.</p>\n<p>So how do we eliminate all this complexity? With Docker's Multi-Stage Builds.</p>\n<h2>The Solution</h2>\n<p>Multi-Stage Builds helps optimize Dockerfiles while keeping them easy to read and maintain. Docker briefly describes this feature like this:</p>\n<blockquote>\n<p>With multi-stage builds, you use multiple FROM statements in your Dockerfile. Each FROM instruction can use a different base, and each of them begins a new stage of the build. You can selectively copy artifacts from one stage to another, leaving behind everything you don't want in the final image.</p>\n</blockquote>\n<p>Let's break this down. With this feature, a single Dockerfile can now contain multiple stages. A stage is a section of the Dockerfile that starts with a <code>FROM</code> instruction.</p>\n<pre><code>FROM node:12.13.0-alpine as install\nWORKDIR /install\nCOPY package.json /install/\nRUN npm install --quiet\n</code></pre>\n<p>The first line signifies the lines it precedes as a single stage. The instruction as install names this stage install. You don't have to name the stage but it helps in later stages to reference this stage by its name rather than the stage number; in this case zero since it is the first stage in Dockerfile. In this stage we:</p>\n<ul>\n<li>set the working directory to <code>/install</code></li>\n<li>copy the <code>package.json</code> to this directory</li>\n<li>install the dependencies listed in the <code>package.json</code></li>\n</ul>\n<p>This stage is clear. It installs dependencies. We could stop and build this image if we wanted to but the final Docker image is not complete. We're still missing a lot of stages. Let's build the next stage.</p>\n<pre><code>FROM node:12.13.0-alpine as compile\nWORKDIR /build\nCOPY tsconfig.json index.ts /build/\nCOPY --from=install /install/node_modules /build/node_modules\nRUN node_modules/typescript/bin/tsc\n</code></pre>\n<p>We've named this stage the compile stage. In this stage we:</p>\n<ul>\n<li>set the working directory to <code>/build</code></li>\n<li>copy the Typescript source code and compilation file into this directory</li>\n<li>copy the dependencies in the <code>/install/node_modules</code> directory from the install stage to the <code>/build/node_modules</code> directory</li>\n<li>compile the Typescript source code using the <code>tsc</code> compiler</li>\n</ul>\n<p>The main take away from this stage is the <code>COPY --from=&#x3C;stage></code> instruction. This does exactly as it says. It copies files from one stage into the current stage.\nWe now have two stages, <code>install</code> and <code>compile</code>. I'd like to pause here and explain another interesting feature of Multi-Stage Builds. When building an image you can tell Docker what stage to build up to. For example, if we ran <code>docker build --target install .</code> we'd only build the <code>install</code> stage. However, if we did not provide the <code>--target &#x3C;stage></code> it would build all the stages. Okay let's resume developing all the stages.</p>\n<p>Here is the final Dockerfile:</p>\n<pre><code>FROM node:12.13.0-alpine as install\nWORKDIR /install\nCOPY package.json /install/\nRUN npm install --quiet\n\nFROM node:12.13.0-alpine as compile\nWORKDIR /build\nCOPY tsconfig.json index.ts /build/\nCOPY --from=install /install/node_modules /build/node_modules\nRUN node_modules/typescript/bin/tsc\n\nFROM node:12.13.0-alpine as source\nWORKDIR /app\nCOPY --from=compile /build/index.js /app/index.js\nCOPY package.json /app\nRUN npm install --production --quiet\n\nFROM node:12.13.0-alpine as dev\nWORKDIR /app\nCOPY --from=source /app/index.js .\nCOPY --from=source /app/node_modules node_modules\nVOLUME [ \"/watch\" ]\nENTRYPOINT [ \"node\" ]\nCMD [\"index.js\"]\n\nFROM node:12.13.0-alpine as prod\nWORKDIR /app\nCOPY --from=source /app/index.js .\nCOPY --from=source /app/node_modules node_modules\nRUN mkdir /watch\nENTRYPOINT [ \"node\" ]\nCMD [\"index.js\"]\n</code></pre>\n<p>Let's briefly describe the last three stages:</p>\n<ul>\n<li>in the <code>source</code> stage we copy the the source code from the previous stage and install production dependencies.</li>\n<li>in the <code>dev</code> stage we copy the source code from the previous stage, add a volume, set the entry point and command. The volume allows us to test the program running in the Docker image. If we build up to this stage we can test the image by adding to or removing files from the volume on the host and assert that the program is working as expected.</li>\n<li>in the <code>prod</code> stage we copy the source code from the <code>source</code> stage, create a directory <code>/watch</code>, and set the entry point and command.</li>\n</ul>\n<p>The final Docker image on my system is ~80MB. That's nearly half the size of the first Docker image. If we wanted we could apply a few more changes to further reduce the size:\nbuild the base image from scratch and have complete control of the dependencies required to run our program\ncompile the source code to a binary executable</p>\n<h2>Conclusion</h2>\n<p>We've come along way in improving our Dockerfiles and Docker images. Thanks to Multi-Stage Builds:</p>\n<ul>\n<li>we've converted the many error-prone hacky (<code>&#x26;&#x26;</code> and <code>\\</code>) and suboptimal steps in our Dockerfile into multiple stages.</li>\n<li>each stage provided clarity because it was concise.</li>\n<li>copying artifacts from prior stages was simple. No intermediate Docker images were required to be built to disk.</li>\n<li>each stage could be built into a Docker image. You can use this to test each stage.</li>\n<li>the final Docker image became leaner.</li>\n</ul>\n<p>So take a look at your current Dockerfiles and assess its complexity. Analyze how you can convert the many instructions and commands into a Multi-Stage Build. There is no better feeling than making Dockerfiles and Docker images maintainable.</p>\n<p>🤙 Mahalo for reading!</p>","frontmatter":{"title":"Use Docker Multi-Stage Builds for Leaner Docker Images","date":"2019-11-18 08:00","tags":["docker"]}}},"pageContext":{}}